{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#loadDB\n",
    "client = MongoClient()\n",
    "db = client.uva_FODS\n",
    "tweetsdb = db.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Database \n",
    "Filter only US english tweets and store to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = {\n",
    "    \"$and\":[ {\"place.country_code\":\"US\"}, { \"lang\": \"en\" } ]\n",
    "    }\n",
    "#we are keeping only our fields of interest\n",
    "columns_query = {\n",
    "    'text':1,\n",
    "    'entities.hashtags':1,\n",
    "    'entities.user_mentions':1,\n",
    "    'place.full_name':1,\n",
    "    'place.bounding_box':1\n",
    "}\n",
    "\n",
    "tweets = pd.DataFrame(list(tweetsdb.find(\n",
    "    filter_query,\n",
    "    columns_query\n",
    "    )))#.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\n\\n@realDonaldTrump \\n#TrumpPence \\n\\nhttps://t.co/5GMNZq40V3\n",
       "1    #CNN #newday clear #Trump deliberately throwing this race,in 2007 he knew that #ISIS and destabilization of Mideast started w/Iraq invasion \n",
       "2    @realDonaldTrump, you wouldn't recognize a lie if it came from your own mouth, and they do continually. #NeverTrump https://t.co/pKSQM8yikm \n",
       "3    \"Kid, you know, suing someone? Thats the most beautiful thing 1 human being could do to another human being\" @funnyordie @realDonaldTrumpðŸ˜‚ðŸ’©s\n",
       "4    @HillaryClinton you ARE the co-founder of ISIS, you crooked, evil, lying, witch. How can you live with yourself?                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "-Tokenize\n",
    "-Lemmatize\n",
    "-Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "\n",
    "stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.extend(['trump','hillary','clinton', \n",
    "                   'american', 'republican', 'amp', 'usa']) #not needed for topic extraction\n",
    "\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.29461073875427\n"
     ]
    }
   ],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def preprocess_text(tweet):\n",
    "    cleaned_tweet_text =\"\"\n",
    "    result = re.sub(r\"\\@\\w+\" , \"\", tweet) #remove hashtags\n",
    "    result = re.sub(r\"\\#\\w+\" , \"\", result) #remove mentions\n",
    "    result = re.sub(r\"http\\S+\", \"\", result) #remove links\n",
    "    result = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", result) #remove numbers\n",
    "    # cleaned_tweet_text = punct_tokenizer.tokenize(result.lower())\n",
    "    cleaned_tweet_text = regexp_tokenizer.tokenize(result.lower())\n",
    "    cleaned_tweet_text = [token for token in cleaned_tweet_text if token not in stop_words]\n",
    "    cleaned_tweet_text = [get_lemma(token) for token in cleaned_tweet_text]\n",
    "    cleaned_tweet_text = [token for token in cleaned_tweet_text]\n",
    "    finalized_tweet_text = []\n",
    "    for word in cleaned_tweet_text:\n",
    "        if (word is not '') and (len(word)>4 or word.lower() in stopwords.words('english')):\n",
    "            finalized_tweet_text.append(word)\n",
    "    return finalized_tweet_text\n",
    "\n",
    "start_time = time.time()\n",
    "tweets['cleanText'] = tweets['text'].apply(lambda x: preprocess_text(x)) #replace values \n",
    "\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517724"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['cleanText'] = tweets['cleanText'].apply(lambda x: remove_stopwords(x))\n",
    "tweets['cleanText'] .to_pickle('data/LDA/clean_data_for_LDA_' +str(len(tweets['text'])) +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
